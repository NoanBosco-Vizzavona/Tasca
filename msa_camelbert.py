# -*- coding: utf-8 -*-
"""MSA_CAMELBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cpaWwDv0XUlnn2_RkCaAZVSGjbOWfO14
"""

!pip install datasets -q
!pip install transformers -q
!pip install sentencepiece -q
!pip install accelerate -U -q
!pip install pytorch-lightning==1.9.4
!pip install kaldialign

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import torch
import re
from tqdm import tqdm
# %matplotlib inline
import random
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import TrainingArguments, Trainer
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch
import sys
import pytorch_lightning as pl

training_path = "/content/train_final_tashkeel (1).csv"
testing_path = "/content/test_final_tashkeel (1).csv"
validation_path = "/content/validation_final_tashkeel (1).csv"

df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)

print(df_train.head())
print(df_test.head())
print(df_val.head())

# Supprimer les lignes contenant des NaN dans les datasets
df_train_clean = df_train.dropna()
df_test_clean = df_test.dropna()
df_val_clean = df_val.dropna()

# Sauvegarder les datasets modifi√©s sans NaN
train_clean_path = "train_final_tashkeel_cleaned.csv"
test_clean_path = "test_final_tashkeel_cleaned.csv"
val_clean_path = "validation_final_tashkeel_cleaned.csv"

df_train_clean.to_csv(train_clean_path, index=False)
df_test_clean.to_csv(test_clean_path, index=False)
df_val_clean.to_csv(val_clean_path, index=False)

print("‚úÖ Nettoyage termin√© ! Les nouveaux fichiers sans NaN sont sauvegard√©s sous :")
print(f"- {train_clean_path}")
print(f"- {test_clean_path}")
print(f"- {val_clean_path}")

# Afficher un aper√ßu des fichiers nettoy√©

training_path = "train_final_tashkeel_cleaned.csv"
testing_path = "test_final_tashkeel_cleaned.csv"
validation_path = "validation_final_tashkeel_cleaned.csv"
df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)
print(df_train.head())
print(df_test.head())
print(df_val.head())

import pandas as pd

# Charger les datasets nettoy√©s
training_path = "train_final_tashkeel_cleaned.csv"
testing_path = "test_final_tashkeel_cleaned.csv"
validation_path = "validation_final_tashkeel_cleaned.csv"

df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)

# Compter les labels pour chaque dataset
train_counts = df_train["label"].value_counts()
test_counts = df_test["label"].value_counts()
val_counts = df_val["label"].value_counts()

# Fusionner les r√©sultats
total_counts = train_counts.add(test_counts, fill_value=0).add(val_counts, fill_value=0).astype(int)

# Afficher les r√©sultats
print("üìä R√©partition des labels :\n")
print("Train:")
print(train_counts, "\n")

print("Test:")
print(test_counts, "\n")

print("Validation:")
print(val_counts, "\n")

print("Total (3 datasets combin√©s):")
print(total_counts)

"""

# Recharger les fichiers CSV
training_path = "train_final_tashkeel_cleaned.csv"
testing_path = "test_final_tashkeel_cleaned.csv"
validation_path = "validation_final_tashkeel_cleaned.csv"

df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)

# Fonction pour √©quilibrer un dataframe √† N exemples par label
def balance_dataset(df, n_per_label):
    return df.groupby("label", group_keys=False).apply(lambda x: x.sample(n=min(len(x), n_per_label), random_state=42))

# R√©√©quilibrer les datasets
df_train_balanced = balance_dataset(df_train, 138)
df_test_balanced = balance_dataset(df_test, 50)
df_val_balanced = balance_dataset(df_val, 51)

# Sauvegarder les nouveaux datasets √©quilibr√©s
df_train_balanced.to_csv("train_balanced.csv", index=False)
df_test_balanced.to_csv("test_balanced.csv", index=False)
df_val_balanced.to_csv("validation_balanced.csv", index=False)

# Afficher la r√©partition finale pour confirmation
train_counts = df_train_balanced['label'].value_counts()
test_counts = df_test_balanced['label'].value_counts()
val_counts = df_val_balanced['label'].value_counts()

train_counts, test_counts, val_counts
"""



"""
# Dictionnaire de mapping des labels
label_mapping = {'neg': 0, 'neutral': 1, 'pos': 2}

# Appliquer le mapping en convertissant tout en `str` d'abord
df_train['label'] = df_train['label'].astype(str).str.lower().map(label_mapping)
df_test['label'] = df_test['label'].astype(str).str.lower().map(label_mapping)
df_val['label'] = df_val['label'].astype(str).str.lower().map(label_mapping)

# V√©rifier s'il reste des valeurs non converties
print(df_train['label'].unique())
print(df_test['label'].unique())
print(df_val['label'].unique())
"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment")
model = AutoModelForSequenceClassification.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment")

def preprocess_function(examples):
    return tokenizer(examples['text_catt'], padding='max_length', truncation=True, max_length=512, return_tensors="pt")

# Convertir les datasets en format Dataset de Hugging Face
catt_train_dataset = Dataset.from_pandas(df_train[["text_catt", "label"]])
catt_val_dataset = Dataset.from_pandas(df_val[["text_catt", "label"]])  # Ajout de df_val
catt_test_dataset = Dataset.from_pandas(df_test[["text_catt", "label"]])

# Appliquer le preprocessing sur chaque dataset
encoded_train = catt_train_dataset.map(preprocess_function, batched=True)
encoded_val = catt_val_dataset.map(preprocess_function, batched=True)  # Ajout de l'encodage val
encoded_test = catt_test_dataset.map(preprocess_function, batched=True)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    """Calculates and returns a dictionary of metrics."""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

print(df_train.head())
print(df_test.head())
print(df_val.head())

args = TrainingArguments(
    "sentimentClassification",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=torch.cuda.is_available()  # üîπ Only enable fp16 if a GPU is available
)

accuracy = 0
f1 = 0
precision = 0
recall = 0
i = 0

from transformers import BertForSequenceClassification,Trainer

def get_model():
    return AutoModelForSequenceClassification.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment")

seeds = [66]

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=encoded_train,
        eval_dataset=encoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch
"""
    # üîπ Boucle pour chaque epoch, selon num_train_epochs
    for epoch in range(1, args.num_train_epochs + 1):
        trainer.train()  # Entra√Ænement pour une epoch

        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

        # üîπ T√©l√©chargement imm√©diat apr√®s sauvegarde
        from google.colab import files  # Ajoute cette ligne seulement si tu es sur Google Colab
        files.download(checkpoint_path)"""

!mkdir -p checkpoints_epochs  # Commande Bash pour cr√©er le dossier
for epoch in range(1, args.num_train_epochs + 1):
        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        #files.download(checkpoint_path)

def ppreprocess_function(examples):
    return tokenizer(examples['tweet'], truncation=True, max_length=512, padding='max_length')

ttrain_dataset = Dataset.from_pandas(df_train[["tweet", "label"]])
ttest_dataset = Dataset.from_pandas(df_test[["tweet", "label"]])
tval_dataset = Dataset.from_pandas(df_val[["tweet", "label"]])

eencoded_train = ttrain_dataset.map(ppreprocess_function, batched=True)
eencoded_test = ttest_dataset.map(ppreprocess_function, batched=True)
eencoded_val = tval_dataset.map(ppreprocess_function, batched=True)

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=eencoded_train,
        eval_dataset=eencoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch

"""
    for epoch in range(1, 4):  # Sauvegarde √† chaque epoch (ici 15 epochs)
        trainer.train()  # Entra√Ænement pour une epoch

        # Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        files.download(checkpoint_path)  """

for epoch in range(1, args.num_train_epochs + 1):


        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        #files.download(checkpoint_path)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from sklearn.metrics import classification_report

# Charger le mod√®le fine-tun√©
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_3.pth"  # Chemin du mod√®le
# Use the correct model architecture: AutoModelForSequenceClassification
# and the same pre-trained model used for training
model = AutoModelForSequenceClassification.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment")
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

df_test["predicted_label"] = df_test["tweet"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""#Sans CATT

"""

from sklearn.metrics import classification_report

y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from sklearn.metrics import classification_report

# Charger le mod√®le fine-tun√©
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_4_catt.pth"  # Chemin du mod√®le
# Use the correct model architecture: AutoModelForSequenceClassification
# and the same pre-trained model used for training
model = AutoModelForSequenceClassification.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment")
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

df_test["predicted_label_catt"] = df_test["text_catt"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""#Avec CATT"""

from sklearn.metrics import classification_report


y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label_catt"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)
# -*- coding: utf-8 -*-
"""DziriBert_Algerian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oo8ZuZlrKc_WRzMMBFeYFXke5Poaklqa
"""

!git clone https://github.com/afrisenti-semeval/afrisent-semeval-2023

!wget -O afrisent-semeval-2023/utils.py https://raw.githubusercontent.com/abjadai/catt/main/utils.py
!wget -O afrisent-semeval-2023//bw2ar.py https://raw.githubusercontent.com/abjadai/catt/main/bw2ar.py
!wget -O afrisent-semeval-2023/tashkeel_tokenizer.py https://raw.githubusercontent.com/abjadai/catt/main/tashkeel_tokenizer.py
!wget -O afrisent-semeval-2023/train_catt.py https://raw.githubusercontent.com/abjadai/catt/main/train_catt.py
!wget -O afrisent-semeval-2023/transformer.py https://raw.githubusercontent.com/abjadai/catt/main/transformer.py
!wget -O afrisent-semeval-2023/ed.py https://raw.githubusercontent.com/abjadai/catt/main/ed.py
!wget -O afrisent-semeval-2023/ed_pl.py https://raw.githubusercontent.com/abjadai/catt/main/ed_pl.py
!wget -O afrisent-semeval-2023/xer.py https://raw.githubusercontent.com/abjadai/catt/main/xer.py
!wget -O afrisent-semval-2023/datamar https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv

!wget https://raw.githubusercontent.com/alger-ia/dziribert/main/data/train_sent.csv

!wget  https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv

!pip install datasets -q
!pip install transformers -q
!pip install sentencepiece -q
!pip install accelerate -U -q
!pip install pytorch-lightning==1.9.4
!pip install kaldialign

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import torch
import re
from tqdm import tqdm
# %matplotlib inline
import random
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import TrainingArguments, Trainer
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch
import sys
import pytorch_lightning as pl

# Get the current working directory
current_dir = os.getcwd()

# Construct the path to the directory containing ed_pl.py
ed_pl_dir = os.path.join('./afrisent-semeval-2023/')

# Add the directory to the Python path
sys.path.append(ed_pl_dir)

# Now you can import ed_pl
from ed_pl import TashkeelModel
from tashkeel_tokenizer import TashkeelTokenizer
from utils import remove_non_arabic

df_train_dziri = '/content/train_sent.csv'

# Import pandas
import pandas as pd

# Read the CSV into a Pandas DataFrame
df_train_dziri = pd.read_csv('/content/train_sent.csv')

# Garder uniquement 33% des donn√©es du dataset
df_sampled = df_train_dziri.sample(frac=0.33, random_state=42)  # S√©lection al√©atoire de 33% des lignes

# Sauvegarder le dataset r√©duit
# Sauvegarder le dataset r√©duit en format TSV
reduce_file_path = "train_sent_33_percent.tsv"
df_sampled.to_csv(reduce_file_path, sep="\t", index=False)

print(f"‚úÖ Dataset r√©duit sauvegard√© en TSV : {reduce_file_path}")
print(df_sampled.head())

# Charger le fichier CSV
df_train_dziri = pd.read_csv('/content/train_sent.csv')

# Sauvegarder en format TSV
tsv_file_path = "/content/train_sent.tsv"
df_train_dziri.to_csv(tsv_file_path, sep="\t", index=False)

print(f"‚úÖ Fichier converti en TSV : {tsv_file_path}")


print(f"‚úÖ Dataset r√©duit sauvegard√© en TSV : {tsv_file_path}")
print(df_train_dziri.head())

# Supprimer la colonne "Unnamed: 0" si elle est pr√©sente
if "Unnamed: 0" in df_train_dziri.columns:
    df_train_dziri = df_train_dziri.drop(columns=["Unnamed: 0"])

# Sauvegarder le dataset propre sans la colonne inutile
cleaned_file_path = "train_sent_clea.tsv"
df_train_dziri.to_csv(cleaned_file_path, sep="\t", index=False)

print("Dataset propre sauvegard√© avec succ√®s :", cleaned_file_path)
print(df_train_dziri.head())

data_path = "./afrisent-semeval-2023/data/arq/"

training_path = data_path + 'train.tsv'
validation_path = data_path + 'dev.tsv'
testing_path = data_path + 'test.tsv'

# Verifying the files in the data folder
print("Files in data folder:", os.listdir(data_path))



# D√©finir les chemins des fichiers
file1 = "/content/train_sent_clea.tsv"
file2 = "/content/afrisent-semeval-2023/data/arq/train.tsv"

# V√©rifier si les fichiers existent
if not os.path.exists(file1) or not os.path.exists(file2):
    raise FileNotFoundError("‚ùå Un des fichiers sp√©cifi√©s n'existe pas. V√©rifie les chemins.")
  # Supprimer la colonne "Unnamed: 0" si elle est pr√©sente

df1 = pd.read_csv(file1, sep="\t")  # Charger df1 ici
df2 = pd.read_csv(file2, sep="\t")

if "Unnamed: 0" in df1.columns:
    df1 = df.drop(columns=["Unnamed: 0"])

# Charger les datasets en pr√©cisant le s√©parateur "\t"
df1 = pd.read_csv(file1, sep="\t")
df2 = pd.read_csv(file2, sep="\t")

# Renommer la colonne 'content' en 'tweet' pour que les fichiers aient les m√™mes colonnes
df1.rename(columns={"text": "tweet"}, inplace=True)

# V√©rifier que les colonnes sont bien les m√™mes apr√®s renommage
if list(df1.columns) != list(df2.columns):
    raise ValueError("‚ùå Les colonnes des fichiers ne correspondent toujours pas apr√®s renommage !")

# Concat√©ner les deux datasets
df_final = pd.concat([df1, df2], ignore_index=True)

# Sauvegarder le dataset fusionn√© dans un chemin valide
merged_file_path = "merged_train.tsv"
df_final.to_csv(merged_file_path, sep="\t", index=False)
print(f"‚úÖ Dataset fusionn√© sauvegard√© avec succ√®s dans : {merged_file_path}")

training_path = "/content/merged_train.tsv"
validation_path = data_path + 'dev.tsv'
testing_path = data_path + 'test.tsv'

# Verifying the files in the data folder
print("Files in data folder:", os.listdir(data_path))

# Charger les donn√©es avec Pandas
df_train = pd.read_csv(training_path, sep="\t")
df_val = pd.read_csv(validation_path, sep="\t")
df_test = pd.read_csv(testing_path, sep="\t")

# D√©tection des caract√®res
arabic_chars = re.compile(r'[\u0600-\u06FF]')  # Lettres arabes
latin_chars = re.compile(r'[a-zA-Z]')  # Lettres latines
digits = re.compile(r'\d')  # Chiffres
emoji_pattern = re.compile(
    "[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF"
    "\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002702-\U000027B0]+",
    flags=re.UNICODE
)
arabic_words = re.compile(r'\b[\u0600-\u06FF]+\b')  # Arabic words
latin_words = re.compile(r'\b[a-zA-Z]+\b')  # Latin words

def display_totals(df, name):
    # Check if columns exist before calculating sums
    if 'arabic_chars' not in df.columns or 'latin_chars' not in df.columns or 'digits' not in df.columns or 'emojis' not in df.columns or 'arabic_words' not in df.columns or 'latin_words' not in df.columns or 'total_words' not in df.columns:
        # Add the columns to the dataframe with default values of 0
        for column in ['arabic_chars', 'latin_chars', 'digits', 'emojis', 'arabic_words', 'latin_words', 'total_words']:
            if column not in df.columns:
                df[column] = 0  # Add a new column with default values of 0

    # Now proceed with calculating the sums
    total_arabic_chars = df['arabic_chars'].sum()
    total_latin_chars = df['latin_chars'].sum()
    total_digits = df['digits'].sum()
    total_emojis = df['emojis'].sum()
    total_arabic_words = df['arabic_words'].sum()
    total_latin_words = df['latin_words'].sum()
    total_words = df['total_words'].sum()

    total_counts = pd.DataFrame({
        "Cat√©gorie": ["Caract√®res Arabes", "Caract√®res Latins", "Chiffres", "Emojis", "Mots Arabes", "Mots Latins", "Total Mots"],
        "Total": [total_arabic_chars, total_latin_chars, total_digits, total_emojis, total_arabic_words, total_latin_words, total_words]
    })

    print(f"Totaux pour {name}:")
    print(total_counts)
    print("\n")  # Add a newline for better readability


# Process each dataframe and display its totals
for df, name in [(df_train, "df_train"), (df_val, "df_val"), (df_test, "df_test")]:
    # Iterate through the DataFrame rows to extract features using regex
    for index, row in df.iterrows():
        # Access the text using the new column name 'text'
        text = row['tweet']  # Changed from 'tweet' to 'text'

        # Apply the regular expressions and store results in new columns
        df.loc[index, 'arabic_chars'] = len(arabic_chars.findall(text))
        df.loc[index, 'latin_chars'] = len(latin_chars.findall(text))
        df.loc[index, 'digits'] = len(digits.findall(text))
        df.loc[index, 'emojis'] = len(emoji_pattern.findall(text))
        df.loc[index, 'arabic_words'] = len(arabic_words.findall(text))
        df.loc[index, 'latin_words'] = len(latin_words.findall(text))
        df.loc[index, 'total_words'] = len(text.split())

        total_counts = pd.DataFrame({
          "Cat√©gorie": ["Caract√®res Arabes", "Caract√®res Latins", "Chiffres", "Emojis", "Mots Arabes", "Mots Latins", "Total Mots"],
          "Total": [df['arabic_chars'].sum(), df['latin_chars'].sum(), df['digits'].sum(), df['emojis'].sum(), df['arabic_words'].sum(), df['latin_words'].sum(), df['total_words'].sum()]
    })

    display_totals(df, name)  # Call the function to display totals

# V√©rifier si df_train est bien d√©fini avant de continuer
if 'df_train' in locals():
    # Compter les phrases contenant des lettres latines et celles qui n'en ont pas
    df_train['contains_latin'] = df_train['tweet'].apply(lambda x: bool(latin_chars.search(x)))

    # Calcul des statistiques
    total_with_latin = df_train['contains_latin'].sum()  # Nombre de phrases avec lettres latines
    total_without_latin = len(df_train) - total_with_latin  # Nombre de phrases sans lettres latines
    total_sentences = len(df_train)  # Nombre total de phrases

    # Cr√©ation d'un DataFrame pour afficher les r√©sultats
    latin_stats = pd.DataFrame({
        "Cat√©gorie": ["Phrases avec lettres latines", "Phrases sans lettres latines", "Total de phrases"],
        "Total": [total_with_latin, total_without_latin, total_sentences]
    })

    # Afficher les r√©sultats
    display_totals(df, latin_stats)  # Call the function to display totals

# Apply regex substitution to the 'tweet' column of each DataFrame
df_train['tweet'] = df_train['tweet'].apply(lambda x: re.sub(r'@\S+', ' ', str(x)))
df_val['tweet'] = df_val['tweet'].apply(lambda x: re.sub(r'@user', ' ', str(x)))
df_test['tweet'] = df_test['tweet'].apply(lambda x: re.sub(r'@user', ' ', str(x)))

# Afficher les premi√®res lignes
print(df_train.head())

df_train = df_train[["tweet", "label"]]
print(df_train.head())

!mkdir afrisent-semeval-2023/models/

!wget -O afrisent-semeval-2023/models/modeed https://github.com/abjadai/catt/releases/download/v2/best_ed_mlm_ns_epoch_178.pt
!wget -O afrisent-semeval-2023/models/modeod https://github.com/abjadai/catt/releases/download/v2/best_eo_mlm_ns_epoch_193.pt

# Model's Configs
model_type = 'ed' # 'eo' for Encoder-Only OR 'ed' for Encoder-Decoder
dl_num_workers = 32
batch_size = 32
max_seq_len = 1024
threshold = 0.6

# Pretrained Char-Based BERT
pretrained_mlm_pt = None # Use None if you want to initialize weights randomly OR the path to the char-based BERT
#pretrained_mlm_pt = 'char_bert_model_pretrained.pt'

tokenizer = TashkeelTokenizer()
ckpt_path = '/content/afrisent-semeval-2023/models/modeed'

print('ckpt_path is:', ckpt_path)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('device:', device)

max_seq_len = 1024
print('Creating Model...')
model = TashkeelModel(tokenizer, max_seq_len=max_seq_len, n_layers=3, learnable_pos_emb=False)

model.load_state_dict(torch.load(ckpt_path, map_location=device))
model.eval().to(device)

# list of undiacritized texts
x = ['ŸàŸÇÿßŸÑÿ™ ŸÖÿ¨ŸÑÿ© ŸÜŸäŸàÿ≤ŸàŸäŸÉ ÿßŸÑÿ£ŸÖÿ±ŸäŸÉŸäÿ© ÿßŸÑÿ™ÿ≠ÿØŸäÿ´ ÿßŸÑÿ¨ÿØŸäÿØ ŸÑ ÿ•ŸÜÿ≥ÿ™ÿ¨ÿ±ÿßŸÖ ŸäŸÖŸÉŸÜ ÿ£ŸÜ Ÿäÿ≥ÿßŸáŸÖ ŸÅŸä ÿ•ŸäŸÇÿßŸÅ ŸàŸÉÿ¥ŸÅ  MIAOU Cest LE CHATÿßŸÑÿ≠ÿ≥ÿßÿ®ÿßÿ™ ÿßŸÑŸÖÿ≤Ÿàÿ±ÿ© ÿ®ÿ≥ŸáŸàŸÑÿ© ÿ¥ÿØŸäÿØÿ©']

xe = [remove_non_arabic(i) for i in x]
batch_size = 16
verbose = True
x_tashkeel = model.do_tashkeel_batch(x, batch_size, verbose)

print(x)
print(xe)
print('-'*85)
print(x_tashkeel)

# Import pandas
import pandas as pd

# Read the CSV into a Pandas DataFrame

# Garder uniquement 33% des donn√©es du dataset
df_train_dix = df_train.sample(frac=0.01, random_state=42)  # S√©lection al√©atoire de 10% des lignes

# Sauvegarder le dataset r√©duit
# Sauvegarder le dataset r√©duit en format TSV
reduce_file_path = "train_sent_10_percent.tsv"
df_train_dix.to_csv(reduce_file_path, sep="\t", index=False)

print(f"‚úÖ Dataset r√©duit sauvegard√© en TSV : {reduce_file_path}")
print(df_train_dix.head())

!mkdir -p afrisent-semeval-2023
!wget -O afrisent-semeval-2023/tashkeel_dataset.py https://raw.githubusercontent.com/abjadai/catt/main/tashkeel_dataset.py
!wget -O afrisent-semeval-2023/tashkeel_tokenizer.py https://raw.githubusercontent.com/abjadai/catt/main/tashkeel_tokenizer.py
!wget -O afrisent-semeval-2023/ed_pl.py https://raw.githubusercontent.com/abjadai/catt/main/ed_pl.py
!wget -O afrisent-semeval-2023/eo_pl.py https://raw.githubusercontent.com/abjadai/catt/main/eo_pl.py

import torch

checkpoint_path = "/content/afrisent-semeval-2023/models/modeed"

# Explicitly specifying map_location to handle potential device mismatch (CPU/GPU)
checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'gpu'))

# Print the keys of the checkpoint dictionary to understand its structure
#print(checkpoint.keys()) # uncomment for debugging

# Filter out the unexpected key "pytorch-lightning_version" if present
filtered_checkpoint = {k: v for k, v in checkpoint.items() if k != 'pytorch-lightning_version'}
# Check if the checkpoint has a "pytorch-lightning_version" key, add it if missing
#if 'pytorch-lightning_version' not in checkpoint:
#    checkpoint['pytorch-lightning_version'] = '1.9.4'  # Or the version compatible with your model

# If 'state_dict' is not the correct key, replace it with the actual key containing the model weights
# Based on the checkpoint structure, try this key (if it's present in your checkpoint):
catt_model = TashkeelModel(tokenizer, max_seq_len=max_seq_len, n_layers=3, learnable_pos_emb=False)
catt_model.load_state_dict(filtered_checkpoint)  # Try loading the whole checkpoint after filtering
catt_model.eval()

training_path_red = '/content/merged_train.tsv'

# Charger les donn√©es avec Pandas
df_train = pd.read_csv(training_path_red, sep="\t")
df_val = pd.read_csv(validation_path, sep="\t")
df_test = pd.read_csv(testing_path, sep="\t")

"""
arabic_pattern = re.compile(r'[\u0600-\u06FF]+')  # Mots en arabe

def apply_catt_preserving_latin(text):
    # Trouver les morceaux arabes
    arabic_parts = arabic_pattern.findall(text)

    # Si aucun arabe n'est d√©tect√©, retourner le texte d'origine
    if not arabic_parts:
        return text

    # Appliquer CATT sur chaque morceau arabe
    tashkeelized_parts = [catt_model.do_tashkeel(part) for part in arabic_parts]

    # Remplacer les morceaux arabes d'origine par leur version tashkeelis√©e
    for original, tashkeelized in zip(arabic_parts, tashkeelized_parts):
        text = text.replace(original, tashkeelized)

    return text

# Appliquer la transformation sur le dataset
#df_train["text_catt"] = df_train["tweet"].apply(apply_catt_preserving_latin)
df_val["text_catt"] = df_val["tweet"].apply(apply_catt_preserving_latin)
df_test["text_catt"] = df_test["tweet"].apply(apply_catt_preserving_latin)

# V√©rifier un exemple
print("üìå Texte original :", df_train["tweet"].iloc[1])
print("‚úÖ Texte avec Tashkeel (CATT) et lettres latines conserv√©es :", df_train["text_catt"].iloc[1])
"""

print(df_train.head())

print(df_train.head())

"""
#D√©finir le chemin de sauvegarde pour chaque dataset
train_file_path = "df_train_with_catt.tsv"
val_file_path = "df_val_with_catt.tsv"
test_file_path = "df_test_with_catt.tsv"

# Sauvegarder les datasets avec les colonnes modifi√©es
df_train.to_csv(train_file_path, sep="\t", index=False)
df_val.to_csv(val_file_path, sep="\t", index=False)
df_test.to_csv(test_file_path, sep="\t", index=False)

print(f"‚úÖ Datasets enregistr√©s localement :\n- {train_file_path}\n- {val_file_path}\n- {test_file_path}")

# Afficher un aper√ßu du dataset train avec pandas
print("Dataset Train avec CATT:")
print(df_train.head()) # Afficher les 5 premi√®res lignes
"""

catt_training_path = "/content/df_train_with_catt.tsv"
catt_validation_path = "/content/df_val_with_catt.tsv"
catt_testing_path = "/content/df_test_with_catt.tsv"

# Verifying the files in the data folder

df_train = pd.read_csv(catt_training_path, sep="\t")
df_val = pd.read_csv(catt_validation_path, sep="\t")
df_test = pd.read_csv(catt_testing_path, sep="\t")



print(df_train.head())

from transformers import BertTokenizer, BertForMaskedLM
from transformers import BertForSequenceClassification

tokenizer = BertTokenizer.from_pretrained("alger-ia/dziribert_sentiment")
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")

# Dictionnaire de mapping des labels
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2, '0': 0, '1': 1, '2': 2}

# Appliquer le mapping en convertissant tout en `str` d'abord
df_train['label'] = df_train['label'].astype(str).str.lower().map(label_mapping)
df_test['label'] = df_test['label'].astype(str).str.lower().map(label_mapping)
df_val['label'] = df_val['label'].astype(str).str.lower().map(label_mapping)

# V√©rifier s'il reste des valeurs non converties
print(df_train['label'].unique())
print(df_test['label'].unique())
print(df_val['label'].unique())

print(df_train['label'].value_counts())  # Voir combien de chaque label dans train
print(df_test['label'].value_counts())   # Voir combien de chaque label dans test
print(df_val['label'].value_counts())    # Voir combien de chaque label dans val

print(df_train.head())

def preprocess_function(examples):
    return tokenizer(examples['text_catt'], padding='max_length', truncation=True, max_length=512, return_tensors="pt")

# Convertir les datasets en format Dataset de Hugging Face
catt_train_dataset = Dataset.from_pandas(df_train[["text_catt", "label"]])
catt_val_dataset = Dataset.from_pandas(df_val[["text_catt", "label"]])  # Ajout de df_val
catt_test_dataset = Dataset.from_pandas(df_test[["text_catt", "label"]])

# Appliquer le preprocessing sur chaque dataset
encoded_train = catt_train_dataset.map(preprocess_function, batched=True)
encoded_val = catt_val_dataset.map(preprocess_function, batched=True)  # Ajout de l'encodage val
encoded_test = catt_test_dataset.map(preprocess_function, batched=True)

"""
# Fusionner les datasets de validation et de test en un seul
encoded_eval = concatenate_datasets([encoded_val, encoded_test])"""

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    """Calculates and returns a dictionary of metrics."""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

args = TrainingArguments(
    "sentimentClassification",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=torch.cuda.is_available()  # üîπ Only enable fp16 if a GPU is available
)

accuracy = 0
f1 = 0
precision = 0
recall = 0
i = 0

from transformers import BertForSequenceClassification,Trainer

def get_model():
    return BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")

seeds = [66]

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=encoded_train,
        eval_dataset=encoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch
"""
    # üîπ Boucle pour chaque epoch, selon num_train_epochs
    for epoch in range(1, args.num_train_epochs + 1):
        trainer.train()  # Entra√Ænement pour une epoch

        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

        # üîπ T√©l√©chargement imm√©diat apr√®s sauvegarde
        from google.colab import files  # Ajoute cette ligne seulement si tu es sur Google Colab
        files.download(checkpoint_path)"""

!mkdir -p checkpoints_epochs  # Commande Bash pour cr√©er le dossier


    for epoch in range(1, args.num_train_epochs + 1):


        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

        # üîπ T√©l√©chargement imm√©diat apr√®s sauvegarde

files.download(checkpoint_path)

def ppreprocess_function(examples):
    return tokenizer(examples['tweet'], truncation=True, max_length=512, padding='max_length')

ttrain_dataset = Dataset.from_pandas(df_train[["tweet", "label"]])
ttest_dataset = Dataset.from_pandas(df_test[["tweet", "label"]])
tval_dataset = Dataset.from_pandas(df_val[["tweet", "label"]])

eencoded_train = ttrain_dataset.map(ppreprocess_function, batched=True)
eencoded_test = ttest_dataset.map(ppreprocess_function, batched=True)
eencoded_val = tval_dataset.map(ppreprocess_function, batched=True)

"""# Fusionner les datasets de validation et de test en un seul
eencoded_eval = concatenate_datasets([eencoded_val, eencoded_test])"""

print(df_val.head())

print(df_train.head())

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=eencoded_train,
        eval_dataset=eencoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch

"""
    for epoch in range(1, 4):  # Sauvegarde √† chaque epoch (ici 15 epochs)
        trainer.train()  # Entra√Ænement pour une epoch

        # Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        files.download(checkpoint_path)  """

for epoch in range(1, args.num_train_epochs + 1):


        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

        # üîπ T√©l√©chargement imm√©diat apr√®s sauvegarde
        from google.colab import files  # Ajoute cette ligne seulement si tu es sur Google Colab

from transformers import BertTokenizer, BertForSequenceClassification
import torch
from sklearn.metrics import classification_report

# Charger le mod√®le fine-tun
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_9.pth"  # Chemin du mod√®le
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")  # Charger l'architecture avec le nom du mod√®le pr√©-entra√Æn√©
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

df_test["predicted_label"] = df_test["tweet"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""# DZIRI BERT SANS CATT

"""

from sklearn.metrics import classification_report

y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)

"""#Chargement de Dziri+catt"""

# Charger le mod√®le fine-tun√©
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_8_catt.pth"  # Chemin du mod√®le
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")  # Charger l'architecture avec le nom du mod√®le pr√©-entra√Æn√©
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

print(df_test.head())

df_test["predicted_label_catt"] = df_test["text_catt"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""# DZIRI BERT AVEC CATT"""

from sklearn.metrics import classification_report


y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label_catt"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)
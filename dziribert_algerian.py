# -*- coding: utf-8 -*-
"""DziriBert_Algerian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oo8ZuZlrKc_WRzMMBFeYFXke5Poaklqa
"""

!git clone https://github.com/afrisenti-semeval/afrisent-semeval-2023

!wget -O afrisent-semeval-2023/utils.py https://raw.githubusercontent.com/abjadai/catt/main/utils.py
!wget -O afrisent-semeval-2023//bw2ar.py https://raw.githubusercontent.com/abjadai/catt/main/bw2ar.py
!wget -O afrisent-semeval-2023/tashkeel_tokenizer.py https://raw.githubusercontent.com/abjadai/catt/main/tashkeel_tokenizer.py
!wget -O afrisent-semeval-2023/train_catt.py https://raw.githubusercontent.com/abjadai/catt/main/train_catt.py
!wget -O afrisent-semeval-2023/transformer.py https://raw.githubusercontent.com/abjadai/catt/main/transformer.py
!wget -O afrisent-semeval-2023/ed.py https://raw.githubusercontent.com/abjadai/catt/main/ed.py
!wget -O afrisent-semeval-2023/ed_pl.py https://raw.githubusercontent.com/abjadai/catt/main/ed_pl.py
!wget -O afrisent-semeval-2023/xer.py https://raw.githubusercontent.com/abjadai/catt/main/xer.py
!wget -O afrisent-semval-2023/datamar https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv

!wget https://raw.githubusercontent.com/alger-ia/dziribert/main/data/train_sent.csv

!wget  https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv

!pip install datasets -q
!pip install transformers -q
!pip install sentencepiece -q
!pip install accelerate -U -q
!pip install pytorch-lightning==1.9.4
!pip install kaldialign

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import torch
import re
from tqdm import tqdm
# %matplotlib inline
import random
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import TrainingArguments, Trainer
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch
import sys
import pytorch_lightning as pl

# Get the current working directory
current_dir = os.getcwd()

# Construct the path to the directory containing ed_pl.py
ed_pl_dir = os.path.join('./afrisent-semeval-2023/')

# Add the directory to the Python path
sys.path.append(ed_pl_dir)

# Now you can import ed_pl
from ed_pl import TashkeelModel
from tashkeel_tokenizer import TashkeelTokenizer
from utils import remove_non_arabic

df_train_dziri = '/content/train_sent.csv'

# Import pandas
import pandas as pd

# Read the CSV into a Pandas DataFrame
df_train_dziri = pd.read_csv('/content/train_sent.csv')

# Garder uniquement 33% des données du dataset
df_sampled = df_train_dziri.sample(frac=0.33, random_state=42)  # Sélection aléatoire de 33% des lignes

# Sauvegarder le dataset réduit
# Sauvegarder le dataset réduit en format TSV
reduce_file_path = "train_sent_33_percent.tsv"
df_sampled.to_csv(reduce_file_path, sep="\t", index=False)

print(f"✅ Dataset réduit sauvegardé en TSV : {reduce_file_path}")
print(df_sampled.head())

# Charger le fichier CSV
df_train_dziri = pd.read_csv('/content/train_sent.csv')

# Sauvegarder en format TSV
tsv_file_path = "/content/train_sent.tsv"
df_train_dziri.to_csv(tsv_file_path, sep="\t", index=False)

print(f"✅ Fichier converti en TSV : {tsv_file_path}")


print(f"✅ Dataset réduit sauvegardé en TSV : {tsv_file_path}")
print(df_train_dziri.head())

# Supprimer la colonne "Unnamed: 0" si elle est présente
if "Unnamed: 0" in df_train_dziri.columns:
    df_train_dziri = df_train_dziri.drop(columns=["Unnamed: 0"])

# Sauvegarder le dataset propre sans la colonne inutile
cleaned_file_path = "train_sent_clea.tsv"
df_train_dziri.to_csv(cleaned_file_path, sep="\t", index=False)

print("Dataset propre sauvegardé avec succès :", cleaned_file_path)
print(df_train_dziri.head())

data_path = "./afrisent-semeval-2023/data/arq/"

training_path = data_path + 'train.tsv'
validation_path = data_path + 'dev.tsv'
testing_path = data_path + 'test.tsv'

# Verifying the files in the data folder
print("Files in data folder:", os.listdir(data_path))



# Définir les chemins des fichiers
file1 = "/content/train_sent_clea.tsv"
file2 = "/content/afrisent-semeval-2023/data/arq/train.tsv"

# Vérifier si les fichiers existent
if not os.path.exists(file1) or not os.path.exists(file2):
    raise FileNotFoundError("❌ Un des fichiers spécifiés n'existe pas. Vérifie les chemins.")
  # Supprimer la colonne "Unnamed: 0" si elle est présente

df1 = pd.read_csv(file1, sep="\t")  # Charger df1 ici
df2 = pd.read_csv(file2, sep="\t")

if "Unnamed: 0" in df1.columns:
    df1 = df.drop(columns=["Unnamed: 0"])

# Charger les datasets en précisant le séparateur "\t"
df1 = pd.read_csv(file1, sep="\t")
df2 = pd.read_csv(file2, sep="\t")

# Renommer la colonne 'content' en 'tweet' pour que les fichiers aient les mêmes colonnes
df1.rename(columns={"text": "tweet"}, inplace=True)

# Vérifier que les colonnes sont bien les mêmes après renommage
if list(df1.columns) != list(df2.columns):
    raise ValueError("❌ Les colonnes des fichiers ne correspondent toujours pas après renommage !")

# Concaténer les deux datasets
df_final = pd.concat([df1, df2], ignore_index=True)

# Sauvegarder le dataset fusionné dans un chemin valide
merged_file_path = "merged_train.tsv"
df_final.to_csv(merged_file_path, sep="\t", index=False)
print(f"✅ Dataset fusionné sauvegardé avec succès dans : {merged_file_path}")

training_path = "/content/merged_train.tsv"
validation_path = data_path + 'dev.tsv'
testing_path = data_path + 'test.tsv'

# Verifying the files in the data folder
print("Files in data folder:", os.listdir(data_path))

# Charger les données avec Pandas
df_train = pd.read_csv(training_path, sep="\t")
df_val = pd.read_csv(validation_path, sep="\t")
df_test = pd.read_csv(testing_path, sep="\t")

# Détection des caractères
arabic_chars = re.compile(r'[\u0600-\u06FF]')  # Lettres arabes
latin_chars = re.compile(r'[a-zA-Z]')  # Lettres latines
digits = re.compile(r'\d')  # Chiffres
emoji_pattern = re.compile(
    "[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF"
    "\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002702-\U000027B0]+",
    flags=re.UNICODE
)
arabic_words = re.compile(r'\b[\u0600-\u06FF]+\b')  # Arabic words
latin_words = re.compile(r'\b[a-zA-Z]+\b')  # Latin words

def display_totals(df, name):
    # Check if columns exist before calculating sums
    if 'arabic_chars' not in df.columns or 'latin_chars' not in df.columns or 'digits' not in df.columns or 'emojis' not in df.columns or 'arabic_words' not in df.columns or 'latin_words' not in df.columns or 'total_words' not in df.columns:
        # Add the columns to the dataframe with default values of 0
        for column in ['arabic_chars', 'latin_chars', 'digits', 'emojis', 'arabic_words', 'latin_words', 'total_words']:
            if column not in df.columns:
                df[column] = 0  # Add a new column with default values of 0

    # Now proceed with calculating the sums
    total_arabic_chars = df['arabic_chars'].sum()
    total_latin_chars = df['latin_chars'].sum()
    total_digits = df['digits'].sum()
    total_emojis = df['emojis'].sum()
    total_arabic_words = df['arabic_words'].sum()
    total_latin_words = df['latin_words'].sum()
    total_words = df['total_words'].sum()

    total_counts = pd.DataFrame({
        "Catégorie": ["Caractères Arabes", "Caractères Latins", "Chiffres", "Emojis", "Mots Arabes", "Mots Latins", "Total Mots"],
        "Total": [total_arabic_chars, total_latin_chars, total_digits, total_emojis, total_arabic_words, total_latin_words, total_words]
    })

    print(f"Totaux pour {name}:")
    print(total_counts)
    print("\n")  # Add a newline for better readability


# Process each dataframe and display its totals
for df, name in [(df_train, "df_train"), (df_val, "df_val"), (df_test, "df_test")]:
    # Iterate through the DataFrame rows to extract features using regex
    for index, row in df.iterrows():
        # Access the text using the new column name 'text'
        text = row['tweet']  # Changed from 'tweet' to 'text'

        # Apply the regular expressions and store results in new columns
        df.loc[index, 'arabic_chars'] = len(arabic_chars.findall(text))
        df.loc[index, 'latin_chars'] = len(latin_chars.findall(text))
        df.loc[index, 'digits'] = len(digits.findall(text))
        df.loc[index, 'emojis'] = len(emoji_pattern.findall(text))
        df.loc[index, 'arabic_words'] = len(arabic_words.findall(text))
        df.loc[index, 'latin_words'] = len(latin_words.findall(text))
        df.loc[index, 'total_words'] = len(text.split())

        total_counts = pd.DataFrame({
          "Catégorie": ["Caractères Arabes", "Caractères Latins", "Chiffres", "Emojis", "Mots Arabes", "Mots Latins", "Total Mots"],
          "Total": [df['arabic_chars'].sum(), df['latin_chars'].sum(), df['digits'].sum(), df['emojis'].sum(), df['arabic_words'].sum(), df['latin_words'].sum(), df['total_words'].sum()]
    })

    display_totals(df, name)  # Call the function to display totals

# Vérifier si df_train est bien défini avant de continuer
if 'df_train' in locals():
    # Compter les phrases contenant des lettres latines et celles qui n'en ont pas
    df_train['contains_latin'] = df_train['tweet'].apply(lambda x: bool(latin_chars.search(x)))

    # Calcul des statistiques
    total_with_latin = df_train['contains_latin'].sum()  # Nombre de phrases avec lettres latines
    total_without_latin = len(df_train) - total_with_latin  # Nombre de phrases sans lettres latines
    total_sentences = len(df_train)  # Nombre total de phrases

    # Création d'un DataFrame pour afficher les résultats
    latin_stats = pd.DataFrame({
        "Catégorie": ["Phrases avec lettres latines", "Phrases sans lettres latines", "Total de phrases"],
        "Total": [total_with_latin, total_without_latin, total_sentences]
    })

    # Afficher les résultats
    display_totals(df, latin_stats)  # Call the function to display totals

# Apply regex substitution to the 'tweet' column of each DataFrame
df_train['tweet'] = df_train['tweet'].apply(lambda x: re.sub(r'@\S+', ' ', str(x)))
df_val['tweet'] = df_val['tweet'].apply(lambda x: re.sub(r'@user', ' ', str(x)))
df_test['tweet'] = df_test['tweet'].apply(lambda x: re.sub(r'@user', ' ', str(x)))

# Afficher les premières lignes
print(df_train.head())

df_train = df_train[["tweet", "label"]]
print(df_train.head())

!mkdir afrisent-semeval-2023/models/

!wget -O afrisent-semeval-2023/models/modeed https://github.com/abjadai/catt/releases/download/v2/best_ed_mlm_ns_epoch_178.pt
!wget -O afrisent-semeval-2023/models/modeod https://github.com/abjadai/catt/releases/download/v2/best_eo_mlm_ns_epoch_193.pt

# Model's Configs
model_type = 'ed' # 'eo' for Encoder-Only OR 'ed' for Encoder-Decoder
dl_num_workers = 32
batch_size = 32
max_seq_len = 1024
threshold = 0.6

# Pretrained Char-Based BERT
pretrained_mlm_pt = None # Use None if you want to initialize weights randomly OR the path to the char-based BERT
#pretrained_mlm_pt = 'char_bert_model_pretrained.pt'

tokenizer = TashkeelTokenizer()
ckpt_path = '/content/afrisent-semeval-2023/models/modeed'

print('ckpt_path is:', ckpt_path)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('device:', device)

max_seq_len = 1024
print('Creating Model...')
model = TashkeelModel(tokenizer, max_seq_len=max_seq_len, n_layers=3, learnable_pos_emb=False)

model.load_state_dict(torch.load(ckpt_path, map_location=device))
model.eval().to(device)

# list of undiacritized texts
x = ['وقالت مجلة نيوزويك الأمريكية التحديث الجديد ل إنستجرام يمكن أن يساهم في إيقاف وكشف  MIAOU Cest LE CHATالحسابات المزورة بسهولة شديدة']

xe = [remove_non_arabic(i) for i in x]
batch_size = 16
verbose = True
x_tashkeel = model.do_tashkeel_batch(x, batch_size, verbose)

print(x)
print(xe)
print('-'*85)
print(x_tashkeel)

# Import pandas
import pandas as pd

# Read the CSV into a Pandas DataFrame

# Garder uniquement 33% des données du dataset
df_train_dix = df_train.sample(frac=0.01, random_state=42)  # Sélection aléatoire de 10% des lignes

# Sauvegarder le dataset réduit
# Sauvegarder le dataset réduit en format TSV
reduce_file_path = "train_sent_10_percent.tsv"
df_train_dix.to_csv(reduce_file_path, sep="\t", index=False)

print(f"✅ Dataset réduit sauvegardé en TSV : {reduce_file_path}")
print(df_train_dix.head())

!mkdir -p afrisent-semeval-2023
!wget -O afrisent-semeval-2023/tashkeel_dataset.py https://raw.githubusercontent.com/abjadai/catt/main/tashkeel_dataset.py
!wget -O afrisent-semeval-2023/tashkeel_tokenizer.py https://raw.githubusercontent.com/abjadai/catt/main/tashkeel_tokenizer.py
!wget -O afrisent-semeval-2023/ed_pl.py https://raw.githubusercontent.com/abjadai/catt/main/ed_pl.py
!wget -O afrisent-semeval-2023/eo_pl.py https://raw.githubusercontent.com/abjadai/catt/main/eo_pl.py

import torch

checkpoint_path = "/content/afrisent-semeval-2023/models/modeed"

# Explicitly specifying map_location to handle potential device mismatch (CPU/GPU)
checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'gpu'))

# Print the keys of the checkpoint dictionary to understand its structure
#print(checkpoint.keys()) # uncomment for debugging

# Filter out the unexpected key "pytorch-lightning_version" if present
filtered_checkpoint = {k: v for k, v in checkpoint.items() if k != 'pytorch-lightning_version'}
# Check if the checkpoint has a "pytorch-lightning_version" key, add it if missing
#if 'pytorch-lightning_version' not in checkpoint:
#    checkpoint['pytorch-lightning_version'] = '1.9.4'  # Or the version compatible with your model

# If 'state_dict' is not the correct key, replace it with the actual key containing the model weights
# Based on the checkpoint structure, try this key (if it's present in your checkpoint):
catt_model = TashkeelModel(tokenizer, max_seq_len=max_seq_len, n_layers=3, learnable_pos_emb=False)
catt_model.load_state_dict(filtered_checkpoint)  # Try loading the whole checkpoint after filtering
catt_model.eval()

training_path_red = '/content/merged_train.tsv'

# Charger les données avec Pandas
df_train = pd.read_csv(training_path_red, sep="\t")
df_val = pd.read_csv(validation_path, sep="\t")
df_test = pd.read_csv(testing_path, sep="\t")

"""
arabic_pattern = re.compile(r'[\u0600-\u06FF]+')  # Mots en arabe

def apply_catt_preserving_latin(text):
    # Trouver les morceaux arabes
    arabic_parts = arabic_pattern.findall(text)

    # Si aucun arabe n'est détecté, retourner le texte d'origine
    if not arabic_parts:
        return text

    # Appliquer CATT sur chaque morceau arabe
    tashkeelized_parts = [catt_model.do_tashkeel(part) for part in arabic_parts]

    # Remplacer les morceaux arabes d'origine par leur version tashkeelisée
    for original, tashkeelized in zip(arabic_parts, tashkeelized_parts):
        text = text.replace(original, tashkeelized)

    return text

# Appliquer la transformation sur le dataset
#df_train["text_catt"] = df_train["tweet"].apply(apply_catt_preserving_latin)
df_val["text_catt"] = df_val["tweet"].apply(apply_catt_preserving_latin)
df_test["text_catt"] = df_test["tweet"].apply(apply_catt_preserving_latin)

# Vérifier un exemple
print("📌 Texte original :", df_train["tweet"].iloc[1])
print("✅ Texte avec Tashkeel (CATT) et lettres latines conservées :", df_train["text_catt"].iloc[1])
"""

print(df_train.head())

print(df_train.head())

"""
#Définir le chemin de sauvegarde pour chaque dataset
train_file_path = "df_train_with_catt.tsv"
val_file_path = "df_val_with_catt.tsv"
test_file_path = "df_test_with_catt.tsv"

# Sauvegarder les datasets avec les colonnes modifiées
df_train.to_csv(train_file_path, sep="\t", index=False)
df_val.to_csv(val_file_path, sep="\t", index=False)
df_test.to_csv(test_file_path, sep="\t", index=False)

print(f"✅ Datasets enregistrés localement :\n- {train_file_path}\n- {val_file_path}\n- {test_file_path}")

# Afficher un aperçu du dataset train avec pandas
print("Dataset Train avec CATT:")
print(df_train.head()) # Afficher les 5 premières lignes
"""

catt_training_path = "/content/df_train_with_catt.tsv"
catt_validation_path = "/content/df_val_with_catt.tsv"
catt_testing_path = "/content/df_test_with_catt.tsv"

# Verifying the files in the data folder

df_train = pd.read_csv(catt_training_path, sep="\t")
df_val = pd.read_csv(catt_validation_path, sep="\t")
df_test = pd.read_csv(catt_testing_path, sep="\t")



print(df_train.head())

from transformers import BertTokenizer, BertForMaskedLM
from transformers import BertForSequenceClassification

tokenizer = BertTokenizer.from_pretrained("alger-ia/dziribert_sentiment")
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")

# Dictionnaire de mapping des labels
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2, '0': 0, '1': 1, '2': 2}

# Appliquer le mapping en convertissant tout en `str` d'abord
df_train['label'] = df_train['label'].astype(str).str.lower().map(label_mapping)
df_test['label'] = df_test['label'].astype(str).str.lower().map(label_mapping)
df_val['label'] = df_val['label'].astype(str).str.lower().map(label_mapping)

# Vérifier s'il reste des valeurs non converties
print(df_train['label'].unique())
print(df_test['label'].unique())
print(df_val['label'].unique())

print(df_train['label'].value_counts())  # Voir combien de chaque label dans train
print(df_test['label'].value_counts())   # Voir combien de chaque label dans test
print(df_val['label'].value_counts())    # Voir combien de chaque label dans val

print(df_train.head())

def preprocess_function(examples):
    return tokenizer(examples['text_catt'], padding='max_length', truncation=True, max_length=512, return_tensors="pt")

# Convertir les datasets en format Dataset de Hugging Face
catt_train_dataset = Dataset.from_pandas(df_train[["text_catt", "label"]])
catt_val_dataset = Dataset.from_pandas(df_val[["text_catt", "label"]])  # Ajout de df_val
catt_test_dataset = Dataset.from_pandas(df_test[["text_catt", "label"]])

# Appliquer le preprocessing sur chaque dataset
encoded_train = catt_train_dataset.map(preprocess_function, batched=True)
encoded_val = catt_val_dataset.map(preprocess_function, batched=True)  # Ajout de l'encodage val
encoded_test = catt_test_dataset.map(preprocess_function, batched=True)

"""
# Fusionner les datasets de validation et de test en un seul
encoded_eval = concatenate_datasets([encoded_val, encoded_test])"""

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    """Calculates and returns a dictionary of metrics."""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

args = TrainingArguments(
    "sentimentClassification",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=torch.cuda.is_available()  # 🔹 Only enable fp16 if a GPU is available
)

accuracy = 0
f1 = 0
precision = 0
recall = 0
i = 0

from transformers import BertForSequenceClassification,Trainer

def get_model():
    return BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")

seeds = [66]

# Dossier pour stocker les modèles
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=encoded_train,
        eval_dataset=encoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entraînement pour une epoch
"""
    # 🔹 Boucle pour chaque epoch, selon num_train_epochs
    for epoch in range(1, args.num_train_epochs + 1):
        trainer.train()  # Entraînement pour une epoch

        # 🔹 Sauvegarde du modèle après chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"✅ Modèle sauvegardé : {checkpoint_path}")

        # 🔹 Téléchargement immédiat après sauvegarde
        from google.colab import files  # Ajoute cette ligne seulement si tu es sur Google Colab
        files.download(checkpoint_path)"""

!mkdir -p checkpoints_epochs  # Commande Bash pour créer le dossier


    for epoch in range(1, args.num_train_epochs + 1):


        # 🔹 Sauvegarde du modèle après chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"✅ Modèle sauvegardé : {checkpoint_path}")

        # 🔹 Téléchargement immédiat après sauvegarde

files.download(checkpoint_path)

def ppreprocess_function(examples):
    return tokenizer(examples['tweet'], truncation=True, max_length=512, padding='max_length')

ttrain_dataset = Dataset.from_pandas(df_train[["tweet", "label"]])
ttest_dataset = Dataset.from_pandas(df_test[["tweet", "label"]])
tval_dataset = Dataset.from_pandas(df_val[["tweet", "label"]])

eencoded_train = ttrain_dataset.map(ppreprocess_function, batched=True)
eencoded_test = ttest_dataset.map(ppreprocess_function, batched=True)
eencoded_val = tval_dataset.map(ppreprocess_function, batched=True)

"""# Fusionner les datasets de validation et de test en un seul
eencoded_eval = concatenate_datasets([eencoded_val, eencoded_test])"""

print(df_val.head())

print(df_train.head())

# Dossier pour stocker les modèles
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=eencoded_train,
        eval_dataset=eencoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entraînement pour une epoch

"""
    for epoch in range(1, 4):  # Sauvegarde à chaque epoch (ici 15 epochs)
        trainer.train()  # Entraînement pour une epoch

        # Sauvegarde du modèle après chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"✅ Modèle sauvegardé : {checkpoint_path}")
        files.download(checkpoint_path)  """

for epoch in range(1, args.num_train_epochs + 1):


        # 🔹 Sauvegarde du modèle après chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"✅ Modèle sauvegardé : {checkpoint_path}")

        # 🔹 Téléchargement immédiat après sauvegarde
        from google.colab import files  # Ajoute cette ligne seulement si tu es sur Google Colab

from transformers import BertTokenizer, BertForSequenceClassification
import torch
from sklearn.metrics import classification_report

# Charger le modèle fine-tun
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_9.pth"  # Chemin du modèle
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")  # Charger l'architecture avec le nom du modèle pré-entraîné
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entraînés
model.eval()  # Mode évaluation

def predict_sentiment(text, model, tokenizer):
    """Prédit le sentiment d'un texte (0 = négatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe prédite

    return predicted_class

df_test["predicted_label"] = df_test["tweet"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# Vérifier un aperçu
print(df_test.head())

"""# DZIRI BERT SANS CATT

"""

from sklearn.metrics import classification_report

y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label"].tolist()


report = classification_report(y_true, y_pred, target_names=["Négatif", "Neutre", "Positif"], digits=4)

print(report)

"""#Chargement de Dziri+catt"""

# Charger le modèle fine-tuné
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_8_catt.pth"  # Chemin du modèle
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")  # Charger l'architecture avec le nom du modèle pré-entraîné
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entraînés
model.eval()  # Mode évaluation

def predict_sentiment(text, model, tokenizer):
    """Prédit le sentiment d'un texte (0 = négatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe prédite

    return predicted_class

print(df_test.head())

df_test["predicted_label_catt"] = df_test["text_catt"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# Vérifier un aperçu
print(df_test.head())

"""# DZIRI BERT AVEC CATT"""

from sklearn.metrics import classification_report


y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label_catt"].tolist()


report = classification_report(y_true, y_pred, target_names=["Négatif", "Neutre", "Positif"], digits=4)

print(report)
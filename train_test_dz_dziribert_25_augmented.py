# -*- coding: utf-8 -*-
"""train_test_dz_dziribert_25_augmented

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOMqda39QUBkLPUK0r293027yq7Gb97P
"""

!pip install transformers -q
!pip install sentencepiece -q
!pip install datasets -q
!pip install transformers -q
!pip install sentencepiece -q
!pip install accelerate -U -q
!pip install pytorch-lightning==1.9.4
!pip install kaldialign

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import torch
import re
from tqdm import tqdm
# %matplotlib inline
import random
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import TrainingArguments, Trainer
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch
import sys
import pytorch_lightning as pl

# Recharger les fichiers
df_full = pd.read_csv('/content/df_train_with_catt_DZ1.tsv', sep='\t')
df_10p = pd.read_csv('/content/df_train_with_catt_DZ1_25p.tsv', sep='\t')

# Fusionner les text_catt depuis df_full dans df_10p l√† o√π tweet est identique
df_fixed = df_10p.copy()

# Cr√©er un dictionnaire tweet ‚ûú text_catt depuis l'original complet
tashkeel_dict = dict(zip(df_full['tweet'], df_full['text_catt']))

# Appliquer sur les lignes de df_10p qui n'ont pas encore de Tashkeel
df_fixed['text_catt'] = df_fixed.apply(
    lambda row: tashkeel_dict[row['tweet']] if pd.isna(row['text_catt']) and row['tweet'] in tashkeel_dict else row['text_catt'],
    axis=1
)

# Sauvegarder le dataset r√©par√©
df_fixed.to_csv('df_train_augmented_complete_tashkeel.tsv', sep='\t', index=False)

# Statistiques post-fusion
print("‚úÖ Total avec Tashkeel maintenant :", df_fixed['text_catt'].notna().sum(), "/", len(df_fixed))



training_path = "/content/df_train_augmented_complete_tashkeel.tsv"
testing_path = "/content/df_test_with_catt_DZ1.tsv"
validation_path = "/content/df_val_with_catt_DZ1.tsv"

# Charger les donn√©es avec Pandas
df_train = pd.read_csv(training_path, sep="\t")
df_val = pd.read_csv(validation_path, sep="\t")
df_test = pd.read_csv(testing_path, sep="\t")

# Supprimer l'ancienne colonne text_catt
if 'text_catt' in df_train.columns:
    df_train.drop(columns=['text_catt'], inplace=True)

# Renommer tweet_catt ‚Üí text_catt
df_train.rename(columns={'tweet_catt': 'text_catt'}, inplace=True)

# Sauvegarder
df_train.to_csv('df_train_augmented_final.tsv', sep='\t', index=False)

training_path = "/content/df_train_augmented_complete_tashkeel.tsv"

df_train = pd.read_csv(training_path, sep="\t")
df_val = pd.read_csv(validation_path, sep="\t")
df_test = pd.read_csv(testing_path, sep="\t")

print(df_train.head())
print(df_test.head())
print(df_val.head())

# Supprimer les lignes contenant des NaN dans les datasets
df_train_clean = df_train.dropna()
df_test_clean = df_test.dropna()
df_val_clean = df_val.dropna()

# Sauvegarder les datasets modifi√©s sans NaN
train_clean_path = "train_final_tashkeel_cleaned.csv"
test_clean_path = "test_final_tashkeel_cleaned.csv"
val_clean_path = "validation_final_tashkeel_cleaned.csv"

df_train_clean.to_csv(train_clean_path, index=False)
df_test_clean.to_csv(test_clean_path, index=False)
df_val_clean.to_csv(val_clean_path, index=False)

print("‚úÖ Nettoyage termin√© ! Les nouveaux fichiers sans NaN sont sauvegard√©s sous :")
print(f"- {train_clean_path}")
print(f"- {test_clean_path}")
print(f"- {val_clean_path}")

# Afficher un aper√ßu des fichiers nettoy√©

training_path = "train_final_tashkeel_cleaned.csv"
testing_path = "test_final_tashkeel_cleaned.csv"
validation_path = "validation_final_tashkeel_cleaned.csv"
df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)
print(df_train.head())
print(df_test.head())
print(df_val.head())

import pandas as pd

# Charger les datasets nettoy√©s
training_path = "train_final_tashkeel_cleaned.csv"
testing_path = "test_final_tashkeel_cleaned.csv"
validation_path = "validation_final_tashkeel_cleaned.csv"

df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)

# Compter les labels pour chaque dataset
train_counts = df_train["label"].value_counts()
test_counts = df_test["label"].value_counts()
val_counts = df_val["label"].value_counts()

# Fusionner les r√©sultats
total_counts = train_counts.add(test_counts, fill_value=0).add(val_counts, fill_value=0).astype(int)

# Afficher les r√©sultats
print("üìä R√©partition des labels :\n")
print("Train:")
print(train_counts, "\n")

print("Test:")
print(test_counts, "\n")

print("Validation:")
print(val_counts, "\n")

print("Total (3 datasets combin√©s):")
print(total_counts)

"""

# Recharger les fichiers CSV
training_path = "train_final_tashkeel_cleaned.csv"
testing_path = "test_final_tashkeel_cleaned.csv"
validation_path = "validation_final_tashkeel_cleaned.csv"

df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)

# Fonction pour √©quilibrer un dataframe √† N exemples par label
def balance_dataset(df, n_per_label):
    return df.groupby("label", group_keys=False).apply(lambda x: x.sample(n=min(len(x), n_per_label), random_state=42))

# R√©√©quilibrer les datasets
df_train_balanced = balance_dataset(df_train, 138)
df_test_balanced = balance_dataset(df_test, 50)
df_val_balanced = balance_dataset(df_val, 51)

# Sauvegarder les nouveaux datasets √©quilibr√©s
df_train_balanced.to_csv("train_balanced.csv", index=False)
df_test_balanced.to_csv("test_balanced.csv", index=False)
df_val_balanced.to_csv("validation_balanced.csv", index=False)

# Afficher la r√©partition finale pour confirmation
train_counts = df_train_balanced['label'].value_counts()
test_counts = df_test_balanced['label'].value_counts()
val_counts = df_val_balanced['label'].value_counts()

train_counts, test_counts, val_counts
"""



# Dictionnaire de mapping des labels
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2,'0': 0,'1': 1,'2': 2,}

# Appliquer le mapping en convertissant tout en `str` d'abord
df_train['label'] = df_train['label'].astype(str).str.lower().map(label_mapping)
df_test['label'] = df_test['label'].astype(str).str.lower().map(label_mapping)
df_val['label'] = df_val['label'].astype(str).str.lower().map(label_mapping)

# V√©rifier s'il reste des valeurs non converties
print(df_train['label'].unique())
print(df_test['label'].unique())
print(df_val['label'].unique())

print(df_train.head())

from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = BertTokenizer.from_pretrained("alger-ia/dziribert_sentiment")
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")

def preprocess_function(examples):
    return tokenizer(examples['text_catt'], padding='max_length', truncation=True, max_length=512, return_tensors="pt")

# Convert datasets to Hugging Face Dataset format
catt_train_dataset = Dataset.from_pandas(df_train[["text_catt", "label"]])
catt_val_dataset = Dataset.from_pandas(df_val[["text_catt", "label"]])
catt_test_dataset = Dataset.from_pandas(df_test[["text_catt", "label"]])


# Apply preprocessing to each dataset
encoded_train = catt_train_dataset.map(preprocess_function, batched=True)
encoded_val = catt_val_dataset.map(preprocess_function, batched=True)
encoded_test = catt_test_dataset.map(preprocess_function, batched=True)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    """Calculates and returns a dictionary of metrics."""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

print(df_train.head())
print(df_test.head())
print(df_val.head())

args = TrainingArguments(
    "sentimentClassification",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=torch.cuda.is_available()  # üîπ Only enable fp16 if a GPU is available
)

accuracy = 0
f1 = 0
precision = 0
recall = 0
i = 0

from transformers import BertForSequenceClassification,Trainer

def get_model():
    return AutoModelForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")

seeds = [66]

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=encoded_train,
        eval_dataset=encoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch
"""
    # üîπ Boucle pour chaque epoch, selon num_train_epochs
    for epoch in range(1, args.num_train_epochs + 1):
        trainer.train()  # Entra√Ænement pour une epoch

        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

        # üîπ T√©l√©chargement imm√©diat apr√®s sauvegarde
        from google.colab import files  # Ajoute cette ligne seulement si tu es sur Google Colab
        files.download(checkpoint_path)"""

print("Labels uniques :", set(encoded_train['label']))

!mkdir -p checkpoints_epochs  # Commande Bash pour cr√©er le dossier
for epoch in range(1, args.num_train_epochs + 1):
        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        #files.download(checkpoint_path)

def ppreprocess_function(examples):
    return tokenizer(examples['tweet'], truncation=True, max_length=512, padding='max_length')

ttrain_dataset = Dataset.from_pandas(df_train[["tweet", "label"]])
ttest_dataset = Dataset.from_pandas(df_test[["tweet", "label"]])
tval_dataset = Dataset.from_pandas(df_val[["tweet", "label"]])

eencoded_train = ttrain_dataset.map(ppreprocess_function, batched=True)
eencoded_test = ttest_dataset.map(ppreprocess_function, batched=True)
eencoded_val = tval_dataset.map(ppreprocess_function, batched=True)

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=eencoded_train,
        eval_dataset=eencoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch

"""
    for epoch in range(1, 4):  # Sauvegarde √† chaque epoch (ici 15 epochs)
        trainer.train()  # Entra√Ænement pour une epoch

        # Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        files.download(checkpoint_path)  """

for epoch in range(1, args.num_train_epochs + 1):


        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        #files.download(checkpoint_path)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from sklearn.metrics import classification_report

# Charger le mod√®le fine-tun√©
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_8.pth"  # Chemin du mod√®le
# Use the correct model architecture: AutoModelForSequenceClassification
# and the same pre-trained model used for training
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

df_test["predicted_label"] = df_test["tweet"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""#Sans CATT

"""

from sklearn.metrics import classification_report

y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)



from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from sklearn.metrics import classification_report

# Charger le mod√®le fine-tun√©
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_1_catt.pth"  # Chemin du mod√®le
# Use the correct model architecture: AutoModelForSequenceClassification
# and the same pre-trained model used for training
model = BertForSequenceClassification.from_pretrained("alger-ia/dziribert_sentiment")
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

df_test["predicted_label_catt"] = df_test["text_catt"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""#Avec CATT"""

from sklearn.metrics import classification_report


y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label_catt"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)
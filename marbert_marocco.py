# -*- coding: utf-8 -*-
"""Marbert_Marocco.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zi0kQ-iQAqgFYbj4BBpCa1H3k-mpnqym
"""

!git clone https://github.com/afrisenti-semeval/afrisent-semeval-2023

!pip install datasets -q
!pip install transformers -q
!pip install sentencepiece -q
!pip install accelerate -U -q
!pip install pytorch-lightning==1.9.4
!pip install kaldialign

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import torch
import re
from tqdm import tqdm
# %matplotlib inline
import random
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import TrainingArguments, Trainer
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch
import sys
import pytorch_lightning as pl

training_path = "/content/train_final_tashkeel_MAR1.csv"
validation_path = "/content/val_final_tashkeel_MAR1.csv"
testing_path = "/content/test_final_tashkeel_MAR1.csv"

# Lecture des CSV
df_train = pd.read_csv(training_path)
df_test = pd.read_csv(testing_path)
df_val = pd.read_csv(validation_path)

print(df_train.head())

# Apply regex substitution to the 'tweet' column of each DataFrame
df_train['tweet'] = df_train['tweet'].apply(lambda x: re.sub(r'@\S+', ' ', str(x)))
df_val['tweet'] = df_val['tweet'].apply(lambda x: re.sub(r'@user', ' ', str(x)))
df_test['tweet'] = df_test['tweet'].apply(lambda x: re.sub(r'@user', ' ', str(x)))

# Afficher les premi√®res lignes
print(df_train.head())

from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("Ammar-alhaj-ali/arabic-MARBERT-sentiment")
model = AutoModelForSequenceClassification.from_pretrained("Ammar-alhaj-ali/arabic-MARBERT-sentiment")

# Dictionnaire de mapping des labels
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2, '0': 0, '1': 1, '2': 2}

# Appliquer le mapping en convertissant tout en `str` d'abord
df_train['label'] = df_train['label'].astype(str).str.lower().map(label_mapping)
df_test['label'] = df_test['label'].astype(str).str.lower().map(label_mapping)
df_val['label'] = df_val['label'].astype(str).str.lower().map(label_mapping)


print(df_train['label'].unique())
print(df_test['label'].unique())
print(df_val['label'].unique())

print(df_test.head())

def preprocess_function(examples):
    return tokenizer(examples['text_catt'], padding='max_length', truncation=True, max_length=512, return_tensors="pt")

# Convertir les datasets en format Dataset de Hugging Face
catt_train_dataset = Dataset.from_pandas(df_train[["text_catt", "label"]])
catt_val_dataset = Dataset.from_pandas(df_val[["text_catt", "label"]])  # Ajout de df_val
catt_test_dataset = Dataset.from_pandas(df_test[["text_catt", "label"]])

# Appliquer le preprocessing sur chaque dataset
encoded_train = catt_train_dataset.map(preprocess_function, batched=True)
encoded_val = catt_val_dataset.map(preprocess_function, batched=True)  # Ajout de l'encodage val
encoded_test = catt_test_dataset.map(preprocess_function, batched=True)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    """Calculates and returns a dictionary of metrics."""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

args = TrainingArguments(
    "sentimentClassification",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=torch.cuda.is_available()  # üîπ Only enable fp16 if a GPU is available
)

accuracy = 0
f1 = 0
precision = 0
recall = 0
i = 0

from transformers import BertForSequenceClassification,Trainer

def get_model():
    return AutoModelForSequenceClassification.from_pretrained("Ammar-alhaj-ali/arabic-MARBERT-sentiment")

seeds = [66]

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=encoded_train,
        eval_dataset=encoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch
"""
    # üîπ Boucle pour chaque epoch, selon num_train_epochs
    for epoch in range(1, args.num_train_epochs + 1):
        trainer.train()  # Entra√Ænement pour une epoch

        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

        # üîπ T√©l√©chargement imm√©diat apr√®s sauvegarde
        from google.colab import files  # Ajoute cette ligne seulement si tu es sur Google Colab
        files.download(checkpoint_path)"""

!mkdir -p checkpoints_epochs  # Commande Bash pour cr√©er le dossier
for epoch in range(1, args.num_train_epochs + 1):
        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}_catt.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

def ppreprocess_function(examples):
    return tokenizer(examples['tweet'], truncation=True, max_length=512, padding='max_length')

ttrain_dataset = Dataset.from_pandas(df_train[["tweet", "label"]])
ttest_dataset = Dataset.from_pandas(df_test[["tweet", "label"]])
tval_dataset = Dataset.from_pandas(df_val[["tweet", "label"]])

eencoded_train = ttrain_dataset.map(ppreprocess_function, batched=True)
eencoded_test = ttest_dataset.map(ppreprocess_function, batched=True)
eencoded_val = tval_dataset.map(ppreprocess_function, batched=True)

# Dossier pour stocker les mod√®les
checkpoint_dir = "checkpoints_epochs"
os.makedirs(checkpoint_dir, exist_ok=True)

for seed in seeds:
    print(f'Seed {seed} en cours...')
    args.seed = seed

    trainer = Trainer(
        model_init=get_model,
        args=args,
        train_dataset=eencoded_train,
        eval_dataset=eencoded_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()  # Entra√Ænement pour une epoch

"""
    for epoch in range(1, 4):  # Sauvegarde √† chaque epoch (ici 15 epochs)
        trainer.train()  # Entra√Ænement pour une epoch

        # Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")
        files.download(checkpoint_path)  """

for epoch in range(1, args.num_train_epochs + 1):


        # üîπ Sauvegarde du mod√®le apr√®s chaque epoch
        checkpoint_path = f"{checkpoint_dir}/model_seed_{seed}_epoch_{epoch}.pth"
        torch.save(trainer.model.state_dict(), checkpoint_path)
        print(f"‚úÖ Mod√®le sauvegard√© : {checkpoint_path}")

from transformers import BertTokenizer, BertForSequenceClassification
import torch
from sklearn.metrics import classification_report

# Charger le mod√®le fine-tun
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_9.pth"  # Chemin du mod√®le
model = BertForSequenceClassification.from_pretrained("Ammar-alhaj-ali/arabic-MARBERT-sentiment")  # Charger l'architecture avec le nom du mod√®le pr√©-entra√Æn√©
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

df_test["predicted_label"] = df_test["tweet"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""#MARBERT

"""

from sklearn.metrics import classification_report

y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)

# Charger le mod√®le fine-tun√©
checkpoint_path = "./checkpoints_epochs/model_seed_66_epoch_9_catt.pth"  # Chemin du mod√®le
# üî∏ Use the same model architecture as when you saved the checkpoint
model = BertForSequenceClassification.from_pretrained("Ammar-alhaj-ali/arabic-MARBERT-sentiment")  # Charger l'architecture avec le nom du mod√®le pr√©-entra√Æn√©
model.load_state_dict(torch.load(checkpoint_path))  # Charger les poids entra√Æn√©s
model.eval()  # Mode √©valuation

def predict_sentiment(text, model, tokenizer):
    """Pr√©dit le sentiment d'un texte (0 = n√©gatif, 1 = neutre, 2 = positif)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits).item()  # Classe pr√©dite

    return predicted_class

df_test["predicted_label_catt"] = df_test["text_catt"].apply(lambda x: predict_sentiment(x, model, tokenizer))

# V√©rifier un aper√ßu
print(df_test.head())

"""#MARBERT + CATT miaou"""

from sklearn.metrics import classification_report


y_true = df_test["label"].tolist()
y_pred = df_test["predicted_label_catt"].tolist()


report = classification_report(y_true, y_pred, target_names=["N√©gatif", "Neutre", "Positif"], digits=4)

print(report)

